MODEL:  
  ANCESTRY: 'deit_tiny_patch16_224' 
  TYPE: deit
  NAME: deit_learngene_tiny
  NUM_CLASSES: 100
  DEIT:
    EMBED_DIM: 192
    DEPTHS: 6
    NUM_HEADS: 3
    NUM_HEADS_LEARNGENE: 2
    NUM_HEADS_DESCENDANT: 3
  HDP:
    # HDP (str): whether to use HDP / which type to use, default='' meaning not using
    HDP: 'qkv' # ''
    # HDP_RATIOS (list of int/float): how much to scale down hdp head count | only edit the first number
    HDP_RATIOS: [1.5, 1]  # To set up two linear layers, Linear(num_heads_descendant/HDP_RATIOS[0], num_heads_descendant/HDP_RATIOS[1]) and Linear(num_heads_descendant/HDP_RATIOS[1], num_heads_descendant)
    FFN_RATIOS: 2.0 # 1.3333
    DESCENDANT_FFN_RATIOS: 2.0 # 1.34
    FFN_INHERIT: 'expand' # 'direct'
    # NON_LINEAR (bool): whether to use ReLU and multi HDP layers 
    NON_LINEAR: True
    DISTRIBUTION: 'Gaussian-Layer' # Uniform
  
  # 添加LoRA相关配置
  LORA:
    ENABLED: True
    RANK: 16   # LoRA矩阵的秩
    ALPHA: 32.0  # LoRA的缩放因子
    DROPOUT: 0.1  # LoRA的dropout率
    TARGET_MODULES: ["attn", "mlp", "norm", "head"]  # 要应用LoRA的模块类型
    
DATA:
  # BATCH_SIZE (int): default=128
  BATCH_SIZE: 1200
  DATASET: 'CIFAR100'
  DATA_PATH: '/home/chk/datasets' 
  IMG_SIZE: 32  # CIFAR100的图像尺寸为32x32

TRAIN:
  # lr is for ebs=512, will scale linearly
  # default: warmup_lr (5e-7) -> base_lr (5e-4) -> min_lr (5e-6)
  BASE_LR: 0.002
  WARMUP_LR: 0.005
  MIN_LR: 0.0001
  # WARMUP_EPOCHS (int): default=20
  WARMUP_EPOCHS: 10
  WEIGHT_DECAY: 0.01
  EPOCHS: 200

AUG:
  COLOR_JITTER: 0.3
  AUTO_AUGMENT: 'rand-m9-mstd0.5-inc1'
  REPROB: 0.25
  MIXUP: 0.8
  CUTMIX: 1.0

TAG: 'CIFAR100_tiny_peg_lora' 